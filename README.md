# Human-Interpretable Adversarial Prompt Attack on Large Language Models with Situational Contexts

This repository contains the following:-
1) The codes to attack the different LLMs: gpt-3.5-turbo-0125, phi-1.5, gpt-4, gemma-7b, Meta-Llama-3-8B, and the 4-bit quantized Llama-2 7B chat.
2) [GPT-4 Judge](https://github.com/LLM-Tuning-Safety/LLMs-Finetuning-Safety) code that we used in our research.
3) llama.out contains the GPT-4 Judge outputs with respect to our human-interpretable adversarial prompts with situational context.
